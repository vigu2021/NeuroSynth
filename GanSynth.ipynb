{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vigu2021/NeuroSynth/blob/main/GanSynth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwZac8Pxz3Fz"
      },
      "source": [
        "# Custom GANSynth-Based Note Generator\n",
        "\n",
        "This project is a custom adaptation of the [GANSynth](https://arxiv.org/pdf/1902.08710) architecture for musical note synthesis.\n",
        "\n",
        "We extend the original implementation by conditioning the model on a broader set of features from the NSynth dataset, including:\n",
        "\n",
        "- `pitch` (MIDI, one-hot encoded)\n",
        "- `velocity` (normalized scalar)\n",
        "- `instrument_family` (one-hot encoded)\n",
        "- `instrument_source` (one-hot encoded)\n",
        "- `qualities` (binary vector)\n",
        "\n",
        "\n",
        "The generator learns to produce log-mel spectrograms from a latent vector and conditioning inputs. The audio waveform is recovered via spectrogram inversion.\n",
        "\n",
        "This implementation is based on the original work by the Magenta team:\n",
        "\n",
        "> Engel, J., Hantrakul, L., Gu, C., & Roberts, A. (2019). *GANSynth: Adversarial Neural Audio Synthesis*. ICLR. [arXiv:1902.08710](https://arxiv.org/pdf/1902.08710)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gzbfnjo4CGW"
      },
      "source": [
        " ## Model Pipeline\n",
        "\n",
        "    [ Conditioning Vector + Noise ]\n",
        "            ↓\n",
        "        Generator (GAN)\n",
        "            ↓\n",
        "        Log-Mel Spectrogram\n",
        "            ↓\n",
        "        Inverse Spectrogram Transform (e.g., iSTFT + Griffin-Lim)\n",
        "            ↓\n",
        "        Audio (waveform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fZzxNPsvGhT1"
      },
      "outputs": [],
      "source": [
        "#Import necessary libraries\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from google.colab import files\n",
        "import os\n",
        "from google.cloud import storage\n",
        "import json\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import subprocess\n",
        "import tarfile\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "HEn8o-ukHkB7",
        "outputId": "89ebe4d9-296f-4921-ffc6-4ffaac9e234a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5c6bec05-98d3-4365-a3bd-fac9ce059ca9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5c6bec05-98d3-4365-a3bd-fac9ce059ca9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving nsynth-455415-069a347092e3.json to nsynth-455415-069a347092e3.json\n"
          ]
        }
      ],
      "source": [
        "#Upload service account json file\n",
        "key_file = files.upload()\n",
        "filename = list(key_file.keys())[0]\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = filename\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "K5nba9UgPXjK",
        "outputId": "3588710d-21cf-4efa-b26a-4ddcf72d465a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 Top-level folders:\n",
            " - file_names/\n",
            " - nsynth-test/\n",
            " - nsynth-train/\n",
            " - nsynth-valid/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nShould print:\\n\\n - nsynth-test/\\n - nsynth-train/\\n - nsynth-valid/\\n\\n '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "client = storage.Client()\n",
        "bucket_name = \"nsynth-data\"\n",
        "bucket = client.bucket(bucket_name)\n",
        "\n",
        "\n",
        "blobs = bucket.list_blobs(prefix=\"\", delimiter=\"/\")\n",
        "print(\"📁 Top-level folders:\")\n",
        "for page in blobs.pages:\n",
        "    for prefix in page.prefixes:\n",
        "        print(\" -\", prefix)\n",
        "\n",
        "\"\"\"\n",
        "Should print:\n",
        "\n",
        " - nsynth-test/\n",
        " - nsynth-train/\n",
        " - nsynth-valid/\n",
        "\n",
        " \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWCzd9_14bjL"
      },
      "source": [
        "#1. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from google.cloud import storage\n",
        "\n",
        "\n",
        "def compress_and_upload_audio(\n",
        "    bucket_name: str,\n",
        "    gcs_input_path: str,\n",
        "    gcs_output_path: str,\n",
        "    target_sample_rate: int = 4000,\n",
        "    split: str = \"train\",\n",
        "):\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "\n",
        "    local_tar_path = f\"/tmp/{split}_source.tar.gz\"\n",
        "    extract_path = f\"/tmp/{split}/unzipped\"\n",
        "    compressed_path = f\"/tmp/{split}/compressed\"\n",
        "    final_tar_path = f\"/tmp/{split}_resampled.tar.gz\"\n",
        "\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "    os.makedirs(compressed_path, exist_ok=True)\n",
        "\n",
        "    # 1. download archive from GCS\n",
        "    print(\"📥  Downloading tar.gz from GCS …\")\n",
        "    bucket.blob(gcs_input_path).download_to_filename(local_tar_path)\n",
        "\n",
        "    # 2. extract\n",
        "    print(\"📦  Extracting archive …\")\n",
        "    with tarfile.open(local_tar_path, \"r:gz\") as tar:\n",
        "        tar.extractall(path=extract_path)\n",
        "\n",
        "    # 2.5 locate examples.json (metadata)\n",
        "    meta_path = next(\n",
        "        (p for p in Path(extract_path).rglob(\"examples.json\")), None\n",
        "    )\n",
        "    if meta_path is None:\n",
        "        raise FileNotFoundError(\"❌  examples.json not found inside archive\")\n",
        "\n",
        "    with open(meta_path, \"r\") as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    # 3. resample .wav files (only MIDI 24‑84)\n",
        "    print(\"🎧  Resampling .wav files …\")\n",
        "    files_resampled = 0\n",
        "    resampler_cache = {}\n",
        "\n",
        "    for wav_file in Path(extract_path).rglob(\"*.wav\"):\n",
        "        file_id = wav_file.stem\n",
        "        if file_id not in metadata:\n",
        "            continue\n",
        "\n",
        "        midi = metadata[file_id][\"pitch\"]   # NSynth key is 'pitch'\n",
        "        if not (24 <= midi <= 84):\n",
        "            continue  # skip notes outside desired range\n",
        "\n",
        "        rel_path = wav_file.relative_to(extract_path)\n",
        "        out_file = Path(compressed_path) / rel_path\n",
        "        out_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        audio, sample_rate = torchaudio.load(str(wav_file))\n",
        "        if sample_rate != target_sample_rate:\n",
        "            if sample_rate not in resampler_cache:\n",
        "                resampler_cache[sample_rate] = T.Resample(\n",
        "                    orig_freq=sample_rate, new_freq=target_sample_rate\n",
        "                )\n",
        "            audio = resampler_cache[sample_rate](audio)\n",
        "\n",
        "        torchaudio.save(\n",
        "            str(out_file),\n",
        "            audio,\n",
        "            target_sample_rate,\n",
        "            encoding=\"PCM_S\",\n",
        "            bits_per_sample=16,\n",
        "        )\n",
        "        files_resampled += 1\n",
        "        if files_resampled % 1000 == 0:\n",
        "            print(f\"   – processed {files_resampled:,} files\")\n",
        "\n",
        "    # 4. re‑tar the resampled files\n",
        "    print(\"📚  Creating new .tar.gz …\")\n",
        "    with tarfile.open(final_tar_path, \"w:gz\") as tar_out:\n",
        "        for file in Path(compressed_path).rglob(\"*\"):\n",
        "            tar_out.add(file, arcname=file.relative_to(compressed_path))\n",
        "\n",
        "    print(\n",
        "        f\"✅  Finished: {files_resampled:,} file(s) resampled \"\n",
        "        f\"and archived at {final_tar_path}\"\n",
        "    )\n",
        "\n",
        "    # 5. upload back to GCS\n",
        "    bucket.blob(gcs_output_path).upload_from_filename(final_tar_path)\n",
        "    print(f\"☁️  Uploaded to gs://{bucket_name}/{gcs_output_path}\")\n"
      ],
      "metadata": {
        "id": "mXoBS7daxKNj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this if only folder not in GCS already\n",
        "'''\n",
        "bucket_name = \"nsynth-data\"\n",
        "gcs_input_path = \"nsynth-train.jsonwav.tar.gz\"\n",
        "gcs_output_path = \"nsynth-train_resampled.jsonwav.tar.gz\"\n",
        "target_sample_rate = 4000\n",
        "split = \"train\"\n",
        "\n",
        "compress_and_upload_audio(bucket_name,gcs_input_path,gcs_output_path,target_sample_rate,split)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MEeDKla7866f",
        "outputId": "e1b0cc7f-f020-4a23-f0c4-4ca4ffc29a4d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥  Downloading tar.gz from GCS …\n",
            "📦  Extracting archive …\n",
            "🎧  Resampling .wav files …\n",
            "   – processed 1,000 files\n",
            "   – processed 2,000 files\n",
            "   – processed 3,000 files\n",
            "   – processed 4,000 files\n",
            "   – processed 5,000 files\n",
            "   – processed 6,000 files\n",
            "   – processed 7,000 files\n",
            "   – processed 8,000 files\n",
            "   – processed 9,000 files\n",
            "   – processed 10,000 files\n",
            "   – processed 11,000 files\n",
            "   – processed 12,000 files\n",
            "   – processed 13,000 files\n",
            "   – processed 14,000 files\n",
            "   – processed 15,000 files\n",
            "   – processed 16,000 files\n",
            "   – processed 17,000 files\n",
            "   – processed 18,000 files\n",
            "   – processed 19,000 files\n",
            "   – processed 20,000 files\n",
            "   – processed 21,000 files\n",
            "   – processed 22,000 files\n",
            "   – processed 23,000 files\n",
            "   – processed 24,000 files\n",
            "   – processed 25,000 files\n",
            "   – processed 26,000 files\n",
            "   – processed 27,000 files\n",
            "   – processed 28,000 files\n",
            "   – processed 29,000 files\n",
            "   – processed 30,000 files\n",
            "   – processed 31,000 files\n",
            "   – processed 32,000 files\n",
            "   – processed 33,000 files\n",
            "   – processed 34,000 files\n",
            "   – processed 35,000 files\n",
            "   – processed 36,000 files\n",
            "   – processed 37,000 files\n",
            "   – processed 38,000 files\n",
            "   – processed 39,000 files\n",
            "   – processed 40,000 files\n",
            "   – processed 41,000 files\n",
            "   – processed 42,000 files\n",
            "   – processed 43,000 files\n",
            "   – processed 44,000 files\n",
            "   – processed 45,000 files\n",
            "   – processed 46,000 files\n",
            "   – processed 47,000 files\n",
            "   – processed 48,000 files\n",
            "   – processed 49,000 files\n",
            "   – processed 50,000 files\n",
            "   – processed 51,000 files\n",
            "   – processed 52,000 files\n",
            "   – processed 53,000 files\n",
            "   – processed 54,000 files\n",
            "   – processed 55,000 files\n",
            "   – processed 56,000 files\n",
            "   – processed 57,000 files\n",
            "   – processed 58,000 files\n",
            "   – processed 59,000 files\n",
            "   – processed 60,000 files\n",
            "   – processed 61,000 files\n",
            "   – processed 62,000 files\n",
            "   – processed 63,000 files\n",
            "   – processed 64,000 files\n",
            "   – processed 65,000 files\n",
            "   – processed 66,000 files\n",
            "   – processed 67,000 files\n",
            "   – processed 68,000 files\n",
            "   – processed 69,000 files\n",
            "   – processed 70,000 files\n",
            "   – processed 71,000 files\n",
            "   – processed 72,000 files\n",
            "   – processed 73,000 files\n",
            "   – processed 74,000 files\n",
            "   – processed 75,000 files\n",
            "   – processed 76,000 files\n",
            "   – processed 77,000 files\n",
            "   – processed 78,000 files\n",
            "   – processed 79,000 files\n",
            "   – processed 80,000 files\n",
            "   – processed 81,000 files\n",
            "   – processed 82,000 files\n",
            "   – processed 83,000 files\n",
            "   – processed 84,000 files\n",
            "   – processed 85,000 files\n",
            "   – processed 86,000 files\n",
            "   – processed 87,000 files\n",
            "   – processed 88,000 files\n",
            "   – processed 89,000 files\n",
            "   – processed 90,000 files\n",
            "   – processed 91,000 files\n",
            "   – processed 92,000 files\n",
            "   – processed 93,000 files\n",
            "   – processed 94,000 files\n",
            "   – processed 95,000 files\n",
            "   – processed 96,000 files\n",
            "   – processed 97,000 files\n",
            "   – processed 98,000 files\n",
            "   – processed 99,000 files\n",
            "   – processed 100,000 files\n",
            "   – processed 101,000 files\n",
            "   – processed 102,000 files\n",
            "   – processed 103,000 files\n",
            "   – processed 104,000 files\n",
            "   – processed 105,000 files\n",
            "   – processed 106,000 files\n",
            "   – processed 107,000 files\n",
            "   – processed 108,000 files\n",
            "   – processed 109,000 files\n",
            "   – processed 110,000 files\n",
            "   – processed 111,000 files\n",
            "   – processed 112,000 files\n",
            "   – processed 113,000 files\n",
            "   – processed 114,000 files\n",
            "   – processed 115,000 files\n",
            "   – processed 116,000 files\n",
            "   – processed 117,000 files\n",
            "   – processed 118,000 files\n",
            "   – processed 119,000 files\n",
            "   – processed 120,000 files\n",
            "   – processed 121,000 files\n",
            "   – processed 122,000 files\n",
            "   – processed 123,000 files\n",
            "   – processed 124,000 files\n",
            "   – processed 125,000 files\n",
            "   – processed 126,000 files\n",
            "   – processed 127,000 files\n",
            "   – processed 128,000 files\n",
            "   – processed 129,000 files\n",
            "   – processed 130,000 files\n",
            "   – processed 131,000 files\n",
            "   – processed 132,000 files\n",
            "   – processed 133,000 files\n",
            "   – processed 134,000 files\n",
            "   – processed 135,000 files\n",
            "   – processed 136,000 files\n",
            "   – processed 137,000 files\n",
            "   – processed 138,000 files\n",
            "   – processed 139,000 files\n",
            "   – processed 140,000 files\n",
            "   – processed 141,000 files\n",
            "   – processed 142,000 files\n",
            "   – processed 143,000 files\n",
            "   – processed 144,000 files\n",
            "   – processed 145,000 files\n",
            "   – processed 146,000 files\n",
            "   – processed 147,000 files\n",
            "   – processed 148,000 files\n",
            "   – processed 149,000 files\n",
            "   – processed 150,000 files\n",
            "   – processed 151,000 files\n",
            "   – processed 152,000 files\n",
            "   – processed 153,000 files\n",
            "   – processed 154,000 files\n",
            "   – processed 155,000 files\n",
            "   – processed 156,000 files\n",
            "   – processed 157,000 files\n",
            "   – processed 158,000 files\n",
            "   – processed 159,000 files\n",
            "   – processed 160,000 files\n",
            "   – processed 161,000 files\n",
            "   – processed 162,000 files\n",
            "   – processed 163,000 files\n",
            "   – processed 164,000 files\n",
            "   – processed 165,000 files\n",
            "   – processed 166,000 files\n",
            "   – processed 167,000 files\n",
            "   – processed 168,000 files\n",
            "   – processed 169,000 files\n",
            "   – processed 170,000 files\n",
            "   – processed 171,000 files\n",
            "   – processed 172,000 files\n",
            "   – processed 173,000 files\n",
            "   – processed 174,000 files\n",
            "   – processed 175,000 files\n",
            "   – processed 176,000 files\n",
            "   – processed 177,000 files\n",
            "   – processed 178,000 files\n",
            "   – processed 179,000 files\n",
            "   – processed 180,000 files\n",
            "   – processed 181,000 files\n",
            "   – processed 182,000 files\n",
            "   – processed 183,000 files\n",
            "   – processed 184,000 files\n",
            "   – processed 185,000 files\n",
            "   – processed 186,000 files\n",
            "   – processed 187,000 files\n",
            "   – processed 188,000 files\n",
            "   – processed 189,000 files\n",
            "   – processed 190,000 files\n",
            "   – processed 191,000 files\n",
            "   – processed 192,000 files\n",
            "   – processed 193,000 files\n",
            "   – processed 194,000 files\n",
            "   – processed 195,000 files\n",
            "   – processed 196,000 files\n",
            "   – processed 197,000 files\n",
            "   – processed 198,000 files\n",
            "   – processed 199,000 files\n",
            "   – processed 200,000 files\n",
            "   – processed 201,000 files\n",
            "   – processed 202,000 files\n",
            "   – processed 203,000 files\n",
            "   – processed 204,000 files\n",
            "   – processed 205,000 files\n",
            "   – processed 206,000 files\n",
            "   – processed 207,000 files\n",
            "   – processed 208,000 files\n",
            "   – processed 209,000 files\n",
            "   – processed 210,000 files\n",
            "   – processed 211,000 files\n",
            "   – processed 212,000 files\n",
            "   – processed 213,000 files\n",
            "   – processed 214,000 files\n",
            "   – processed 215,000 files\n",
            "   – processed 216,000 files\n",
            "   – processed 217,000 files\n",
            "   – processed 218,000 files\n",
            "   – processed 219,000 files\n",
            "   – processed 220,000 files\n",
            "   – processed 221,000 files\n",
            "   – processed 222,000 files\n",
            "   – processed 223,000 files\n",
            "📚  Creating new .tar.gz …\n",
            "✅  Finished: 223,775 file(s) resampled and archived at /tmp/train_resampled.tar.gz\n",
            "☁️  Uploaded to gs://nsynth-data/nsynth-train_resampled.jsonwav.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "from google.cloud import storage\n",
        "\n",
        "def download_dataset(gcs_input_path, split, bucket_name=\"nsynth-data\"):\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "\n",
        "    local_tar_path = f\"/tmp/{split}_source.tar.gz\"\n",
        "    extract_path = f\"/tmp/{split}/unzipped\"\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "    # 1. Download the tar.gz file from GCS\n",
        "    print(\"✅ Starting download!\")\n",
        "    bucket.blob(gcs_input_path).download_to_filename(local_tar_path)\n",
        "\n",
        "    # 2. Extract contents to target path\n",
        "    with tarfile.open(local_tar_path, \"r:gz\") as tar:\n",
        "        tar.extractall(path=extract_path)\n",
        "\n",
        "    print(f\"✅ Downloaded and extracted {gcs_input_path} to {extract_path}\")\n",
        "    return extract_path\n",
        "\n",
        "\n",
        "def json_file(json_path,split,bucket_name = \"nsynth-data\"):\n",
        "  client = storage.Client()\n",
        "  bucket = client.bucket(bucket_name)\n",
        "\n",
        "  local_json_path = f\"/tmp/{split}.json\"\n",
        "  bucket.blob(json_path).download_to_filename(local_json_path)\n",
        "  print(f\"✅ Downloaded {json_path} to {local_json_path}\")\n",
        "  return local_json_path\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PYbqRYwSnlZD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gcs_input_path = \"nsynth-train_resampled.jsonwav.tar.gz\"\n",
        "split = \"train\"\n",
        "bucket_name = \"nsynth-data\"\n",
        "extract_path = download_dataset(gcs_input_path, split, bucket_name)\n",
        "\n",
        "\n",
        "json_input_path =\"nsynth-train/examples.json\"\n",
        "split = \"train\"\n",
        "bucket_name = \"nsynth-data\"\n",
        "json_path = json_file(json_input_path,split,bucket_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgzEkRZqpNh9",
        "outputId": "78477b48-6cde-4c2d-f6e7-115f4461b46d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Downloaded nsynth-train/examples.json to /tmp/train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Mr879v-0ZWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "yld-V-Z6NSZa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torchaudio\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NSynthDataset(Dataset):\n",
        "    def __init__(self, split=\"train\", local_cache_dir=\"/tmp/\"):\n",
        "        super().__init__()\n",
        "        self.root_dir = Path(local_cache_dir) / split / \"unzipped\"\n",
        "\n",
        "        # 1. Load the JSON labels\n",
        "        label_path = Path(local_cache_dir) / f\"{split}.json\"\n",
        "        if not label_path.exists():\n",
        "            raise FileNotFoundError(f\"Label file not found: {label_path}\")\n",
        "        with open(label_path, \"r\") as f:\n",
        "            self.labels = json.load(f)\n",
        "\n",
        "        # 2. Scan all WAV files once and map stem → full path\n",
        "        wav_paths = list(self.root_dir.rglob(\"*.wav\"))\n",
        "        if not wav_paths:\n",
        "            raise FileNotFoundError(f\"No .wav files under {self.root_dir}\")\n",
        "        self.wav_map = {p.stem: p for p in wav_paths}\n",
        "\n",
        "        # 3. Filter labels to only those with existing WAVs\n",
        "        all_keys = list(self.labels.keys())\n",
        "        self.file_names = [fn for fn in all_keys if fn in self.wav_map]\n",
        "        skipped = len(all_keys) - len(self.file_names)\n",
        "        if skipped:\n",
        "            print(f\"⚠️  Skipped {skipped} label(s) without audio files\")\n",
        "\n",
        "        # 4. Velocity mapping\n",
        "        self.velocity_map = {25: 0, 50: 1, 75: 2, 100: 3, 127: 4}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fn = self.file_names[idx]\n",
        "        wav_path = self.wav_map[fn]\n",
        "\n",
        "        # Load audio\n",
        "        audio, sr = torchaudio.load(str(wav_path))\n",
        "\n",
        "        # Fetch metadata\n",
        "        m = self.labels[fn]\n",
        "        return {\n",
        "            \"pitch\":             torch.tensor(m[\"pitch\"], dtype=torch.long),\n",
        "            \"velocity\":          torch.tensor(self.velocity_map[int(m[\"velocity\"])], dtype=torch.long),\n",
        "            \"instrument_family\": torch.tensor(m[\"instrument_family\"], dtype=torch.long),\n",
        "            \"instrument_source\": torch.tensor(m[\"instrument_source\"], dtype=torch.long),\n",
        "            \"qualities\":         torch.tensor(m[\"qualities\"], dtype=torch.long),\n",
        "            \"audio\":             audio,\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXy6dmBAr8zk",
        "outputId": "a263910e-510d-4bd7-910a-ef1d2058941f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️  Skipped 65430 label(s) without audio files\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(53)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "#Test Dataset and data loader\n",
        "dataset = NSynthDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True,num_workers = 2, pin_memory = True)\n",
        "batch = next(iter(dataloader))\n",
        "batch[\"pitch\"][0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWb9n9gCb4cC"
      },
      "source": [
        "##2. Create the GANsynth Model\n",
        "\n",
        "## 🎛 Embedding-Based Conditioning for GAN\n",
        "\n",
        "This model uses **learned embeddings** and encoded binary vectors to represent conditioning features from the NSynth dataset. Each feature is embedded or projected into a lower-dimensional continuous space and concatenated to form a high-dimensional vector. This vector conditions both the **generator** and **discriminator** networks.\n",
        "\n",
        "### 🔢 Feature Embedding Dimensions\n",
        "\n",
        "| Feature               | Value Range                | Representation Type     | Output Dimension |\n",
        "|------------------------|-----------------------------|--------------------------|------------------|\n",
        "| **Pitch**             | 0–127 (128 classes)         | Embedding                | 18               |\n",
        "| **Velocity**          | {25, 50, 75, 100, 127} (5 classes)| Embedding                | 3                |\n",
        "| **Instrument Family** | 0–10 (11 classes)           | Embedding                | 5                |\n",
        "| **Instrument Source** | 0–2 (3 classes)             | Embedding                | 3                |\n",
        "| **Qualities**         | 10-dim binary vector        | Linear projection (`10 → 10`) | 10              |\n",
        "\n",
        "- All categorical features use **`nn.Embedding(num_classes, embed_dim)`**.\n",
        "- The embedding dimensions are determined using the following heuristic formula:\n",
        "\n",
        "> 📐 **Embedding Dimension = ⌈1.6 × √(num_classes)⌉**\n",
        "\n",
        "- `Qualities` is a multi-label binary vector (e.g., distorted, bright, percussive).\n",
        "- It is projected to 10 dimensions using a small feedforward layer:  \n",
        "  `projected_qualities = Linear(10, 10)(qualities)`\n",
        "\n",
        "### 📐 Total Conditioning Vector Size\n",
        "\n",
        "All conditioning features are concatenated into one vector:\n",
        "\n",
        "**18 (pitch) + 3 (velocity) + 5 (instrument family) + 3 (instrument source) + 10 (qualities) = 39 dimensions**\n",
        "\n",
        "This **39-dimensional conditioning vector** is concatenated with the latent noise vector `z` and passed into the generator and discriminator networks, enabling the GAN to synthesize high-quality, controllable musical notes that reflect both physical and perceptual attributes of sound.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNSVpvTA0zTO"
      },
      "source": [
        "#2.1  PixelNorm (Pixel-wise Feature Normalization)\n",
        "\n",
        "PixelNorm is a normalization layer used in GANSynth generators\n",
        "It operates **per pixel** and normalizes the **channel-wise feature vector** to have **unit average squared magnitude**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📐 Formula\n",
        "\n",
        "For a 4D input tensor `x` of shape `(B, C, H, W)`, the PixelNorm formula is:\n",
        "$$\n",
        "[\n",
        "\\text{PixelNorm}(x_{b,c,h,w}) = \\frac{x_{b,c,h,w}}{\\sqrt{\\frac{1}{C} \\sum_{c=1}^{C} x_{b,c,h,w}^2 + \\epsilon}}\n",
        "]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DxoUrK7WRYfY"
      },
      "outputs": [],
      "source": [
        "# PixelNorm block\n",
        "# It normalizes each pixel by considering all channels at that pixel location.\n",
        "# The normalization ensures that the **average variance across channels** for each pixel is ≈ 1.\n",
        "\n",
        "class PixelNorm(nn.Module):\n",
        "  def __init__(self,epsilon = 1e-8):\n",
        "    super().__init__()\n",
        "    self.epsilon = epsilon\n",
        "  def forward(self, x):\n",
        "    return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngwlsl9yGnIK"
      },
      "source": [
        "## 🔼 Upsampling ×2 Block (GANSynth Style)\n",
        "\n",
        "This block is a core component in the generator architecture. It progressively increases the spatial resolution (height × width) of the feature map while refining features through convolutional layers.\n",
        "\n",
        "### 🧱 Block Structure\n",
        "\n",
        "The Upsampling ×2 Block consists of the following:\n",
        "\n",
        "1. **Nearest-Neighbor Upsampling**  \n",
        "   - Doubles both the height and width of the feature map  \n",
        "   - e.g., (H, W) → (2H, 2W)\n",
        "\n",
        "2. **Two Conv2D Layers**  \n",
        "   - Each with:\n",
        "     - `kernel_size = 3`\n",
        "     - `stride = 1`\n",
        "     - `padding = 1`\n",
        "\n",
        "\n",
        "### 🔁 Full Layer Sequence\n",
        "\n",
        "```plaintext\n",
        "Input (B, C, H, W)\n",
        "↓\n",
        "Upsample (scale_factor=2) → (B, C, 2H, 2W)\n",
        "↓\n",
        "Conv2D (3×3, stride=1, padding=1) → (B, C_out, 2H, 2W)\n",
        "↓\n",
        "PixelNorm\n",
        "↓\n",
        "LeakyReLU\n",
        "↓\n",
        "Conv2D (3×3, stride=1, padding=1) → (B, C_out, 2H, 2W)\n",
        "↓\n",
        "PixelNorm\n",
        "↓\n",
        "LeakyReLU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "yGPUD9oT5_Ei"
      },
      "outputs": [],
      "source": [
        "#Doubles the width and height by factor of 2\n",
        "\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.norm1 = PixelNorm()\n",
        "        self.act1 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.norm2 = PixelNorm()\n",
        "        self.act2 = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upsample(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.act1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.act2(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t3lzBHpBy0K"
      },
      "source": [
        "##2.2 Generator Architecture Implementation\n",
        "\n",
        "\n",
        "The generator progressively upsamples a low-dimensional latent vector `z`, conditioned on attributes such as pitch, instrument family, velocity, and qualities. It uses:\n",
        "\n",
        "- **Nearest-neighbor upsampling**\n",
        "- **2D convolutional blocks**\n",
        "- **PixelNorm** normalization\n",
        "- **LeakyReLU** activations\n",
        "\n",
        "Each block approximately doubles the resolution of the spectrogram representation, ending in a 2-channel output (real and imaginary components).\n",
        "\n",
        "📷 **The diagram of the GANSynth generator architecture will be used as a reference for implementation.**\n",
        "\n",
        "## 🎛️ GANSynth Generator Architecture - Taken from original paper\n",
        "\n",
        "| Layer Description      | Output Size          | Kernel Size | Filters | Nonlinearity       |\n",
        "|------------------------|----------------------|-------------|---------|--------------------|\n",
        "| concat(Z, Pitch)       | (1, 1, 317)          | -           | -       | -                  |\n",
        "| Conv2D                 | (2, 16, 256)         | 2 × 16      | 256     | PN + LeakyReLU     |\n",
        "| Conv2D                 | (2, 16, 256)         | 3 × 3       | 256     | PN + LeakyReLU     |\n",
        "| Upsample (2×2)         | (4, 32, 256)         | -           | -       | -                  |\n",
        "| Conv2D                 | (4, 32, 256)         | 3 × 3       | 256     | PN + LeakyReLU     |\n",
        "| Conv2D                 | (4, 32, 256)         | 3 × 3       | 256     | PN + LeakyReLU     |\n",
        "| Upsample (2×2)         | (8, 64, 256)         | -           | -       | -                  |\n",
        "| Conv2D                 | (8, 64, 256)         | 3 × 3       | 256     | PN + LeakyReLU     |\n",
        "| Conv2D                 | (8, 64, 256)         | 3 × 3       | 256     | PN + LeakyReLU     |\n",
        "| Upsample (2×2)         | (16, 128, 256)       | -           | -       | -                  |\n",
        "| Conv2D                 | (16, 128, 256)       | 3 × 3       | 256     | PN + LeakyReLU     |\n",
        "| Conv2D                 | (16, 128, 256)       | 3 × 3       | 256     | PN + LeakyReLU     |\n",
        "| Upsample (2×2)         | (32, 256, 256)       | -           | -       | -                  |\n",
        "| Conv2D                 | (32, 256, 128)       | 3 × 3       | 128     | PN + LeakyReLU     |\n",
        "| Conv2D                 | (32, 256, 128)       | 3 × 3       | 128     | PN + LeakyReLU     |\n",
        "| Upsample (2×2)         | (64, 512, 128)       | -           | -       | -                  |\n",
        "| Conv2D                 | (64, 512, 64)        | 3 × 3       | 64      | PN + LeakyReLU     |\n",
        "| Conv2D                 | (64, 512, 64)        | 3 × 3       | 64      | PN + LeakyReLU     |\n",
        "| Upsample (2×2)         | (128, 1024, 64)      | -           | -       | -                  |\n",
        "| Conv2D                 | (128, 1024, 32)      | 3 × 3       | 32      | PN + LeakyReLU     |\n",
        "| Conv2D                 | (128, 1024, 32)      | 3 × 3       | 32      | PN + LeakyReLU     |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "XbUyK6bHjJPm"
      },
      "outputs": [],
      "source": [
        "\n",
        "class mini_Generator_v1(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding and projection layers for conditioning\n",
        "        self.pitch_embedding = nn.Embedding(128, 18)\n",
        "        self.velocity_embedding = nn.Embedding(5, 3)\n",
        "        self.instrument_family_embedding = nn.Embedding(11, 5)\n",
        "        self.instrument_source_embedding = nn.Embedding(3, 3)\n",
        "        self.quality_projection = nn.Linear(10, 10)\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.activation = nn.LeakyReLU(0.2)\n",
        "        self.pixel_norm = PixelNorm()\n",
        "\n",
        "        # Initial linear projection to shape (256, 2, 16)\n",
        "        self.initial_linear = nn.Linear(latent_dim + 39, 256 * 2 * 16)\n",
        "        self.initial_conv = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Upsampling stack (progressively increasing H, W)\n",
        "        self.upsample_layers = nn.Sequential(\n",
        "            UpsampleBlock(256, 128),\n",
        "            UpsampleBlock(128, 64),\n",
        "            UpsampleBlock(64, 256),\n",
        "            UpsampleBlock(256, 128),\n",
        "            UpsampleBlock(128, 64),\n",
        "            UpsampleBlock(64, 32)\n",
        "        )\n",
        "\n",
        "        # Final conv and output activation\n",
        "        self.last_conv = nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1)\n",
        "        self.output_activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, batch, z):\n",
        "        # Embeddings and projection\n",
        "        pitch_emb = self.pitch_embedding(batch[\"pitch\"])                     # (B, 18)\n",
        "        velocity_emb = self.velocity_embedding(batch[\"velocity\"])           # (B, 3)\n",
        "        inst_family_emb = self.instrument_family_embedding(batch[\"instrument_family\"])  # (B, 5)\n",
        "        inst_source_emb = self.instrument_source_embedding(batch[\"instrument_source\"])  # (B, 3)\n",
        "        quality_proj = self.quality_projection(batch[\"qualities\"].float())  # (B, 10)\n",
        "\n",
        "        # Combine all conditioning vectors + latent z\n",
        "        cond_vec = torch.cat([\n",
        "            pitch_emb, velocity_emb, inst_family_emb,\n",
        "            inst_source_emb, quality_proj, z\n",
        "        ], dim=1)  # (B, 167)\n",
        "\n",
        "        # Initial projection and conv\n",
        "        x = self.initial_linear(cond_vec)                   # (B, 8192)\n",
        "        x = x.view(x.size(0), 256, 2, 16)                   # (B, 256, 2, 16)\n",
        "        x = self.initial_conv(x)                            # (B, 256, 2, 16)\n",
        "        x = self.pixel_norm(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        # Upsample to final resolution\n",
        "        x = self.upsample_layers(x)                         # → (B, 32, 128, 1024)\n",
        "\n",
        "        # Final conv + output scaling\n",
        "        x = self.last_conv(x)                               # (B, 2, 128, 1024)\n",
        "        x = self.output_activation(x)                       # [-1, 1] range\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "vpe68ZSDj8Tt"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "z = torch.randn(32,128).to(device)\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "generator = mini_Generator_v1().to(device)\n",
        "\n",
        "generator.eval()\n",
        "with torch.inference_mode():\n",
        "  output = generator(batch, z)         # call forward() with batch and"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkEzxBAatz3G",
        "outputId": "a22ad4e8-8487-49c7-a8bd-6d8b112f34db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 2, 128, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "output.shape # Correct shape (32,2,128,1024)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeA8eyT9vZM1"
      },
      "source": [
        "## 2.3 🎚️ Discriminator Architecture\n",
        "\n",
        "The discriminator receives the **generated (or real) spectrogram** from the generator with shape `(128, 1024, 2)`, and progressively **downsamples** it through a series of convolutional blocks. The final outputs are:\n",
        "\n",
        "- A **real/fake score**, represented as a **single scalar value** per sample indicating whether the input is real or generated.\n",
        "- A **conditioning regression head**, which predicts a **39-dimensional embedding vector** representing the original conditioning inputs (e.g., pitch, velocity, instrument family, source, and qualities).\n",
        "\n",
        "\n",
        "### 🧱 Block-wise Architecture\n",
        "\n",
        "| Layer Description         | Output Size         | Kernel Size | Filters | Nonlinearity       |\n",
        "|---------------------------|----------------------|-------------|---------|--------------------|\n",
        "| Input image               | (128, 1024, 2)       | -           | -       | -                  |\n",
        "| Conv2D                    | (128, 1024, 32)      | 1 × 1       | 32      | -                  |\n",
        "| Conv2D                    | (128, 1024, 32)      | 3 × 3       | 32      | LeakyReLU          |\n",
        "| Conv2D                    | (128, 1024, 32)      | 3 × 3       | 32      | LeakyReLU          |\n",
        "| Downsample (stride=2)     | (64, 512, 32)        | -           | -       | -                  |\n",
        "| Conv2D                    | (64, 512, 64)        | 3 × 3       | 64      | LeakyReLU          |\n",
        "| Conv2D                    | (64, 512, 64)        | 3 × 3       | 64      | LeakyReLU          |\n",
        "| Downsample (stride=2)     | (32, 256, 64)        | -           | -       | -                  |\n",
        "| Conv2D                    | (32, 256, 128)       | 3 × 3       | 128     | LeakyReLU          |\n",
        "| Conv2D                    | (32, 256, 128)       | 3 × 3       | 128     | LeakyReLU          |\n",
        "| Downsample (stride=2)     | (16, 128, 128)       | -           | -       | -                  |\n",
        "| Conv2D                    | (16, 128, 256)       | 3 × 3       | 256     | LeakyReLU          |\n",
        "| Conv2D                    | (16, 128, 256)       | 3 × 3       | 256     | LeakyReLU          |\n",
        "| Downsample (stride=2)     | (8, 64, 256)         | -           | -       | -                  |\n",
        "| Conv2D                    | (8, 64, 256)         | 3 × 3       | 256     | LeakyReLU          |\n",
        "| Conv2D                    | (8, 64, 256)         | 3 × 3       | 256     | LeakyReLU          |\n",
        "| Downsample (stride=2)     | (4, 32, 256)         | -           | -       | -                  |\n",
        "| Conv2D                    | (4, 32, 256)         | 3 × 3       | 256     | LeakyReLU          |\n",
        "| Conv2D                    | (4, 32, 256)         | 3 × 3       | 256     | LeakyReLU          |\n",
        "| Downsample (stride=2)     | (2, 16, 256)         | -           | -       | -                  |\n",
        "| **Minibatch Std. Concat** | (2, 16, 257)         | -           | -       | -                  |\n",
        "| Conv2D                    | (2, 16, 256)         | 3 × 3       | 256     | LeakyReLU          |\n",
        "| Conv2D                    | (2, 16, 256)         | 3 × 3       | 256     | LeakyReLU          |\n",
        "| **Pitch classifier**      | (1, 1, 61)           | -           | 61      | Softmax            |\n",
        "| **Discriminator output**  | (1, 1, 1)            | -           | 1       | - (linear)         |\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "iile_Ly1Dp55"
      },
      "outputs": [],
      "source": [
        "class DownsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
        "        super().__init__()\n",
        "        self.downsample = nn.Sequential(\n",
        "            # Downsampling: keep channels, reduce spatial size\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding),  # (B, C, H, W) -> (B, C, H/2, W/2)\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            # Refinement: increase channels, preserve spatial size\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),  # (B, C, H/2, W/2) -> (B, C_out, H/2, W/2)\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.downsample(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "HU2LJMhMBwlM"
      },
      "outputs": [],
      "source": [
        "class mini_Discriminator_v1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # Initial convolutional block\n",
        "    self.initial_block = nn.Sequential(\n",
        "        nn.Conv2d(2, 32, kernel_size=1),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "\n",
        "    # Downsampling stack\n",
        "    self.downsample_block = nn.Sequential(\n",
        "        DownsampleBlock(32, 64),     # 128×1024 → 64×512\n",
        "        DownsampleBlock(64, 128),    # 64×512 → 32×256\n",
        "        DownsampleBlock(128, 256),   # 32×256 → 16×128\n",
        "        DownsampleBlock(256, 256),   # 16×128 → 8×64\n",
        "        DownsampleBlock(256, 256)    # 8×64 → 4×32\n",
        "    )\n",
        "\n",
        "    # Final spatial downsampling\n",
        "    self.final_downsample = nn.Sequential(\n",
        "        nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),  # → 2×16\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "\n",
        "    # Convolution after concatenating std channel\n",
        "    self.final_conv = nn.Sequential(\n",
        "        nn.Conv2d(257, 256, kernel_size=3, padding=1),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "\n",
        "    # Conditioning regressor (outputs 39D vector)\n",
        "    self.cond_regressor = nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d((1, 1)),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(256, 39)\n",
        "    )\n",
        "\n",
        "    # Real/fake score classifier\n",
        "    self.real_classifier = nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d((1, 1)),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(256, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.initial_block(x)\n",
        "    x = self.downsample_block(x)\n",
        "    x = self.final_downsample(x)\n",
        "\n",
        "    # Append mean of std channel across batch\n",
        "    x_std = torch.std(x, dim=0, keepdim=True).mean()\n",
        "    x_std = x_std.expand(x.shape[0], 1, x.shape[2], x.shape[3])\n",
        "    x = torch.cat([x, x_std], dim=1)\n",
        "\n",
        "    x = self.final_conv(x)\n",
        "\n",
        "    x_cond = self.cond_regressor(x)       # → (B, 39)\n",
        "    x_classifier = self.real_classifier(x)  # → (B, 1)\n",
        "\n",
        "    return {\n",
        "    \"condition\": x_cond,       # 39-dimensional conditioning vector\n",
        "    \"real_fake\": x_classifier  # scalar real/fake score\n",
        "    }\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "-UQZM3liCpa4"
      },
      "outputs": [],
      "source": [
        "discriminator = mini_Discriminator_v1().to(device)\n",
        "discriminator.eval()\n",
        "with torch.inference_mode():\n",
        "  final_output = discriminator(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWe68gXwQnVY",
        "outputId": "b64f99d6-6fe8-4372-b26f-1a475cefce4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The conditioning tensor has shape torch.Size([39])\n",
            "The classifier tensor has shape torch.Size([1])\n"
          ]
        }
      ],
      "source": [
        "print(f\"The conditioning tensor has shape {final_output['condition'][0].shape}\")\n",
        "print(f\"The classifier tensor has shape {final_output['real_fake'][0].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OUxOiliW7mm"
      },
      "outputs": [],
      "source": [
        "#Make spectograms\n",
        "\n",
        "#Takes waveform: (B,1,64000) -> ((B,2,128, 1024))\n",
        "def make_spectrogram(waveform, sample_rate=16000):\n",
        "\n",
        "  waveform = waveform.squeeze(1)\n",
        "\n",
        "  # As used in GanSynth\n",
        "  n_fft = 1024\n",
        "  hop_length = 256\n",
        "  n_mels = 128\n",
        "\n",
        "  #Hann window to make audio windows smoother\n",
        "  window = torch.hann_window(n_fft).to(device)\n",
        "\n",
        "  # 1. Apply STFT on waveform`\n",
        "  stft = torch.stft(\n",
        "        input=waveform,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        return_complex=True,\n",
        "        window = window,\n",
        "    )  # shape: (B, F, T)\n",
        "\n",
        "  #2 . Log-Mel Spectogram\n",
        "  mel_filter = torchaudio.transforms.MelScale(\n",
        "    n_mels=n_mels,\n",
        "    sample_rate=sample_rate,\n",
        "    n_stft=stft.shape[1]  # usually 513 if n_fft=1024\n",
        "    ).to(device)\n",
        "\n",
        "  magnitude = abs(stft)\n",
        "  log_mel_spec = torch.log(1e-8 + mel_filter(magnitude))\n",
        "\n",
        "  #3. Phase spectogram\n",
        "  phase = torch.angle(stft)\n",
        "\n",
        "  #4. Unwrapped Phase\n",
        "  diff = torch.diff(phase, dim=-1)\n",
        "  diff_wrapped = (diff + torch.pi) % (2 * torch.pi) - torch.pi\n",
        "  correction = diff_wrapped - diff\n",
        "  correction[(diff_wrapped == -torch.pi) & (diff > 0)] = torch.pi\n",
        "  unwrapped = torch.cumsum(torch.cat([phase[..., :1], diff + correction], dim=-1), dim=-1)\n",
        "\n",
        "  #5. Instaneous phase\n",
        "  dphase = torch.diff(unwrapped, dim=-1)\n",
        "  inst_freq = dphase * sample_rate / (2 * torch.pi * hop_length)\n",
        "  inst_freq = torch.nn.functional.pad(inst_freq, (1, 0))\n",
        "\n",
        "\n",
        "  #6. Resize\n",
        "  log_mel_spec = log_mel_spec.unsqueeze(1)\n",
        "  inst_freq = inst_freq.unsqueeze(1)\n",
        "\n",
        "  log_mel_spec = torch.nn.functional.interpolate(log_mel_spec, size=(128, 1024), mode='bilinear', align_corners=False)\n",
        "  inst_freq = torch.nn.functional.interpolate(inst_freq, size=(128, 1024), mode='bilinear', align_corners=False)\n",
        "\n",
        "  #7. Concat\n",
        "  spectograms = torch.concat((log_mel_spec, inst_freq), dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return spectograms\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udAB8ATlexNY",
        "outputId": "0b60e5c9-e1fe-4acb-ee61-dd1ec18b4819"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 2, 128, 1024])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "audio_test = batch['audio'].to(device)\n",
        "make_spectrogram(audio_test).shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW3VcldgQpn0"
      },
      "source": [
        "## Section 3: Training Loop\n",
        "\n",
        "This section describes the training process for a GAN-based audio synthesis model. The generator is conditioned on musical attributes (pitch, velocity, etc.), and the discriminator is trained to both classify real vs. fake data and predict a semantic embedding vector corresponding to the input pitch. This embedding-based auxiliary supervision helps structure the discriminator’s internal representation space.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 Training Strategy Overview\n",
        "\n",
        "Each training step involves:\n",
        "1. The **discriminator** is trained to:\n",
        "   - Distinguish real from fake data using adversarial loss,\n",
        "   - Predict a 39-dimensional embedding vector corresponding to the input pitch (using MSE loss).\n",
        "2. The **generator** is trained to:\n",
        "   - Fool the discriminator (adversarial loss),\n",
        "   -  Produce outputs that correct pitch embeddings when passed to the discriminator (MSE loss).\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Loss Functions\n",
        "\n",
        "#### 1. **Discriminator Loss**\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_D = \\mathcal{L}_{\\text{adv}}^{\\text{real}} + \\mathcal{L}_{\\text{adv}}^{\\text{fake}} + \\lambda_{\\text{aux}} \\cdot \\mathcal{L}_{\\text{embed}}^{\\text{real}}\n",
        "$$\n",
        "\n",
        "- **Adversarial loss** (binary cross-entropy):\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{adv}} = -[y \\cdot \\log(x) + (1 - y) \\cdot \\log(1 - x)]\n",
        "$$\n",
        "\n",
        "- **Auxiliary embedding loss** (mean squared error):\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{embed}}^{\\text{real}} = \\text{MSE}(D_{\\text{embed}}(x_{\\text{real}}), \\text{PitchEmbed}(pitch_{\\text{true}}))\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Generator Loss**\n",
        "\n",
        "Two versions:\n",
        "\n",
        "- **Basic Generator Loss (default):**\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_G = \\mathcal{L}_{\\text{adv}}^{\\text{fake}}\n",
        "$$\n",
        "\n",
        "- **Extended Generator Loss (with embedding supervision):**\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_G = \\mathcal{L}_{\\text{adv}}^{\\text{fake}} + \\lambda_{\\text{aux}} \\cdot \\mathcal{L}_{\\text{embed}}^{\\text{fake}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{embed}}^{\\text{fake}} = \\text{MSE}(D_{\\text{embed}}(G(z)), \\text{PitchEmbed}(pitch_{\\text{true}}))\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bwzt_OJ7DnT8"
      },
      "outputs": [],
      "source": [
        "#Define training loop\n",
        "def train_loop(generator,discriminator, train_loader, criterion_adv,criterion_embed, optimizer_G,optimizer_D, device, lambda_embed = 1, epochs=200):\n",
        "  generator.train()\n",
        "  discriminator.train()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    generator_loss = 0\n",
        "    discriminator_loss = 0\n",
        "    for batch_idx, cond in enumerate(train_loader):\n",
        "      z = torch.randn(32,128).to(device) #Random noise\n",
        "      cond = {k: v.to(device) for k, v in cond.items()}\n",
        "      waveform_real = cond['audio'].to(device)\n",
        "\n",
        "      #Get embedded_vector\n",
        "      embedded_vector = torch.cat([  # 🔧 changed torch.concat → torch.cat\n",
        "          generator.pitch_embedding(cond['pitch']),\n",
        "          generator.velocity_embedding(cond['velocity']),\n",
        "          generator.instrument_family_embedding(cond['instrument_family']),\n",
        "          generator.instrument_source_embedding(cond['instrument_source']),\n",
        "          generator.quality_projection(cond['qualities'].float()),\n",
        "      ],dim = 1)\n",
        "\n",
        "      embedded_vector_D = embedded_vector.detach()\n",
        "\n",
        "      #Create spectogram\n",
        "      spectogram_real = make_spectrogram(waveform_real)\n",
        "      spectogram_generated = generator(cond, z)\n",
        "\n",
        "      discriminator_real = discriminator(spectogram_real)\n",
        "      discriminator_generated = discriminator(spectogram_generated.detach())\n",
        "      #Train discriminator\n",
        "\n",
        "      discriminator_real_classifier = discriminator_real['real_fake']\n",
        "      discriminator_real_embed = discriminator_real['condition']\n",
        "      discriminator_generated_classifier = discriminator_generated['real_fake']\n",
        "      discriminator_generated_embed = discriminator_generated['condition']\n",
        "      #Generate labels\n",
        "      real_labels = torch.ones_like(discriminator_real_classifier).detach().to(device)\n",
        "      generated_labels = torch.zeros_like(discriminator_generated_classifier).detach().to(device)\n",
        "\n",
        "      #Adversarial loss\n",
        "      loss_adversarial_discriminator = criterion_adv(discriminator_real_classifier,real_labels) + criterion_adv(discriminator_generated_classifier,generated_labels)\n",
        "      #Auxiliary embedding loss\n",
        "      loss_embed_discriminator = criterion_embed(discriminator_real_embed, embedded_vector_D)\n",
        "      total_discriminator_loss = loss_adversarial_discriminator + lambda_embed * loss_embed_discriminator #Total discriminator loss\n",
        "\n",
        "      optimizer_D.zero_grad()\n",
        "      total_discriminator_loss.backward()\n",
        "      optimizer_D.step()\n",
        "      del spectogram_real, discriminator_real, discriminator_generated\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      print(\"Discriminator trained!\")\n",
        "\n",
        "      #Train Generator\n",
        "      discriminator_generated = discriminator(spectogram_generated)  # re-run for G\n",
        "      discriminator_generated_classifier = discriminator_generated['real_fake']  # 🔧 added: re-extract outputs for generator step\n",
        "      discriminator_generated_embed = discriminator_generated['condition']\n",
        "      loss_adversarial_generator = criterion_adv(discriminator_generated_classifier, real_labels)\n",
        "      loss_embed_generator = criterion_embed(discriminator_generated_embed, embedded_vector)\n",
        "      total_generator_loss = loss_adversarial_generator + lambda_embed * loss_embed_generator #Total generator loss\n",
        "\n",
        "      optimizer_G.zero_grad()\n",
        "      total_generator_loss.backward()\n",
        "      optimizer_G.step()\n",
        "      del spectogram_generated, discriminator_generated_embed, discriminator_generated_classifier\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      print(\"Generator trained!\")\n",
        "      #Log generator and discriminator loss\n",
        "      generator_loss += total_generator_loss.item()\n",
        "      discriminator_loss += total_discriminator_loss.item()\n",
        "\n",
        "      print(f\"[Epoch {epoch + 1}/{epochs}] \"\n",
        "              f\"Generator Loss: {generator_loss:.4f} | \"\n",
        "              f\"Discriminator Loss: {discriminator_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2jRpBw821dI"
      },
      "outputs": [],
      "source": [
        "\n",
        "#device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Define models\n",
        "generator = mini_Generator_v1().to(device)\n",
        "discriminator = mini_Discriminator_v1().to(device)\n",
        "\n",
        "#Define loss functions\n",
        "criterion_adv = nn.BCEWithLogitsLoss()\n",
        "criterion_embed = nn.MSELoss()\n",
        "\n",
        "# Define optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
        "\n",
        "#Dataset and Dataloader\n",
        "dataset = NSynthDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True,num_workers = 2, pin_memory = True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        },
        "id": "1Ut-Ruaq220J",
        "outputId": "8b32adfd-bbba-45a7-b466-e202ca82c557",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discriminator trained!\n",
            "Generator trained!\n",
            "[Epoch 1/100] Generator Loss: 1.3814 | Discriminator Loss: 2.0747\n",
            "Discriminator trained!\n",
            "Generator trained!\n",
            "[Epoch 1/100] Generator Loss: 2.7501 | Discriminator Loss: 4.1370\n",
            "Discriminator trained!\n",
            "Generator trained!\n",
            "[Epoch 1/100] Generator Loss: 4.1161 | Discriminator Loss: 6.1967\n",
            "Discriminator trained!\n",
            "Generator trained!\n",
            "[Epoch 1/100] Generator Loss: 5.5401 | Discriminator Loss: 8.3143\n",
            "Discriminator trained!\n",
            "Generator trained!\n",
            "[Epoch 1/100] Generator Loss: 6.8962 | Discriminator Loss: 10.3645\n",
            "Discriminator trained!\n",
            "Generator trained!\n",
            "[Epoch 1/100] Generator Loss: 8.3099 | Discriminator Loss: 12.4731\n",
            "Discriminator trained!\n",
            "Generator trained!\n",
            "[Epoch 1/100] Generator Loss: 9.7137 | Discriminator Loss: 14.5736\n",
            "Discriminator trained!\n",
            "Generator trained!\n",
            "[Epoch 1/100] Generator Loss: 11.0656 | Discriminator Loss: 16.6302\n",
            "Discriminator trained!\n",
            "Generator trained!\n",
            "[Epoch 1/100] Generator Loss: 12.4199 | Discriminator Loss: 18.7131\n",
            "Discriminator trained!\n",
            "Generator trained!\n",
            "[Epoch 1/100] Generator Loss: 13.8446 | Discriminator Loss: 20.8017\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-cfcb5cac5d56>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_loop(generator, discriminator, dataloader, criterion_adv, criterion_embed,\n\u001b[0m\u001b[1;32m      3\u001b[0m            optimizer_G, optimizer_D, device, lambda_embed=1.0, epochs=100)\n",
            "\u001b[0;32m<ipython-input-11-96d915607b3f>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(generator, discriminator, train_loader, criterion_adv, criterion_embed, optimizer_G, optimizer_D, device, lambda_embed, epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mgenerator_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdiscriminator_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Random noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "#Training loop\n",
        "train_loop(generator, discriminator, dataloader, criterion_adv, criterion_embed,\n",
        "           optimizer_G, optimizer_D, device, lambda_embed=1.0, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "WNswbDJ7VbDl"
      },
      "outputs": [],
      "source": [
        "#This is too heavy to run for me. Refer to GanSynth_mini"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qUZ0A4923SL5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMnRqq8u9ftL2zoHjxRBEhi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}