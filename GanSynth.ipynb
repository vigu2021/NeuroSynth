{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vigu2021/NeuroSynth/blob/main/GanSynth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwZac8Pxz3Fz"
      },
      "source": [
        "# Custom GANSynth-Based Note Generator\n",
        "\n",
        "This project is a custom adaptation of the [GANSynth](https://arxiv.org/pdf/1902.08710) architecture for musical note synthesis.\n",
        "\n",
        "We extend the original implementation by conditioning the model on a broader set of features from the NSynth dataset, including:\n",
        "\n",
        "- `pitch` (MIDI, one-hot encoded)\n",
        "- `velocity` (normalized scalar)\n",
        "- `instrument_family` (one-hot encoded)\n",
        "- `instrument_source` (one-hot encoded)\n",
        "- `qualities` (binary vector)\n",
        "\n",
        "\n",
        "The generator learns to produce log-mel spectrograms from a latent vector and conditioning inputs. The audio waveform is recovered via spectrogram inversion.\n",
        "\n",
        "This implementation is based on the original work by the Magenta team:\n",
        "\n",
        "> Engel, J., Hantrakul, L., Gu, C., & Roberts, A. (2019). *GANSynth: Adversarial Neural Audio Synthesis*. ICLR. [arXiv:1902.08710](https://arxiv.org/pdf/1902.08710)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gzbfnjo4CGW"
      },
      "source": [
        " ## Model Pipeline\n",
        "\n",
        "    [ Conditioning Vector + Noise ]\n",
        "            ‚Üì\n",
        "        Generator (GAN)\n",
        "            ‚Üì\n",
        "        Log-Mel Spectrogram\n",
        "            ‚Üì\n",
        "        Inverse Spectrogram Transform (e.g., iSTFT + Griffin-Lim)\n",
        "            ‚Üì\n",
        "        Audio (waveform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZzxNPsvGhT1"
      },
      "outputs": [],
      "source": [
        "#Import necessary libraries\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from google.colab import files\n",
        "import os\n",
        "from google.cloud import storage\n",
        "import json\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import subprocess\n",
        "import tarfile\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEn8o-ukHkB7"
      },
      "outputs": [],
      "source": [
        "#Upload service account json file\n",
        "key_file = files.upload()\n",
        "filename = list(key_file.keys())[0]\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = filename\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5nba9UgPXjK"
      },
      "outputs": [],
      "source": [
        "client = storage.Client()\n",
        "bucket_name = \"nsynth-data\"\n",
        "bucket = client.bucket(bucket_name)\n",
        "\n",
        "\n",
        "blobs = bucket.list_blobs(prefix=\"\", delimiter=\"/\")\n",
        "print(\"üìÅ Top-level folders:\")\n",
        "for page in blobs.pages:\n",
        "    for prefix in page.prefixes:\n",
        "        print(\" -\", prefix)\n",
        "\n",
        "\"\"\"\n",
        "Should print:\n",
        "\n",
        " - nsynth-test/\n",
        " - nsynth-train/\n",
        " - nsynth-valid/\n",
        "\n",
        " \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWCzd9_14bjL"
      },
      "source": [
        "#1. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from google.cloud import storage\n",
        "\n",
        "\n",
        "def compress_and_upload_audio(\n",
        "    bucket_name: str,\n",
        "    gcs_input_path: str,\n",
        "    gcs_output_path: str,\n",
        "    target_sample_rate: int = 4000,\n",
        "    split: str = \"train\",\n",
        "):\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "\n",
        "    local_tar_path = f\"/tmp/{split}_source.tar.gz\"\n",
        "    extract_path = f\"/tmp/{split}/unzipped\"\n",
        "    compressed_path = f\"/tmp/{split}/compressed\"\n",
        "    final_tar_path = f\"/tmp/{split}_resampled.tar.gz\"\n",
        "\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "    os.makedirs(compressed_path, exist_ok=True)\n",
        "\n",
        "    # 1. download archive from GCS\n",
        "    print(\"üì•  Downloading tar.gz from GCS ‚Ä¶\")\n",
        "    bucket.blob(gcs_input_path).download_to_filename(local_tar_path)\n",
        "\n",
        "    # 2. extract\n",
        "    print(\"üì¶  Extracting archive ‚Ä¶\")\n",
        "    with tarfile.open(local_tar_path, \"r:gz\") as tar:\n",
        "        tar.extractall(path=extract_path)\n",
        "\n",
        "    # 2.5 locate examples.json (metadata)\n",
        "    meta_path = next(\n",
        "        (p for p in Path(extract_path).rglob(\"examples.json\")), None\n",
        "    )\n",
        "    if meta_path is None:\n",
        "        raise FileNotFoundError(\"‚ùå  examples.json not found inside archive\")\n",
        "\n",
        "    with open(meta_path, \"r\") as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    # 3. resample .wav files (only MIDI 24‚Äë84)\n",
        "    print(\"üéß  Resampling .wav files ‚Ä¶\")\n",
        "    files_resampled = 0\n",
        "    resampler_cache = {}\n",
        "\n",
        "    for wav_file in Path(extract_path).rglob(\"*.wav\"):\n",
        "        file_id = wav_file.stem\n",
        "        if file_id not in metadata:\n",
        "            continue\n",
        "\n",
        "        midi = metadata[file_id][\"pitch\"]   # NSynth key is 'pitch'\n",
        "        if not (24 <= midi <= 84):\n",
        "            continue  # skip notes outside desired range\n",
        "\n",
        "        rel_path = wav_file.relative_to(extract_path)\n",
        "        out_file = Path(compressed_path) / rel_path\n",
        "        out_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        audio, sample_rate = torchaudio.load(str(wav_file))\n",
        "        if sample_rate != target_sample_rate:\n",
        "            if sample_rate not in resampler_cache:\n",
        "                resampler_cache[sample_rate] = T.Resample(\n",
        "                    orig_freq=sample_rate, new_freq=target_sample_rate\n",
        "                )\n",
        "            audio = resampler_cache[sample_rate](audio)\n",
        "\n",
        "        torchaudio.save(\n",
        "            str(out_file),\n",
        "            audio,\n",
        "            target_sample_rate,\n",
        "            encoding=\"PCM_S\",\n",
        "            bits_per_sample=16,\n",
        "        )\n",
        "        files_resampled += 1\n",
        "        if files_resampled % 1000 == 0:\n",
        "            print(f\"   ‚Äì processed {files_resampled:,} files\")\n",
        "\n",
        "    # 4. re‚Äëtar the resampled files\n",
        "    print(\"üìö  Creating new .tar.gz ‚Ä¶\")\n",
        "    with tarfile.open(final_tar_path, \"w:gz\") as tar_out:\n",
        "        for file in Path(compressed_path).rglob(\"*\"):\n",
        "            tar_out.add(file, arcname=file.relative_to(compressed_path))\n",
        "\n",
        "    print(\n",
        "        f\"‚úÖ  Finished: {files_resampled:,} file(s) resampled \"\n",
        "        f\"and archived at {final_tar_path}\"\n",
        "    )\n",
        "\n",
        "    # 5. upload back to GCS\n",
        "    bucket.blob(gcs_output_path).upload_from_filename(final_tar_path)\n",
        "    print(f\"‚òÅÔ∏è  Uploaded to gs://{bucket_name}/{gcs_output_path}\")\n"
      ],
      "metadata": {
        "id": "mXoBS7daxKNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this if only folder not in GCS already\n",
        "'''\n",
        "bucket_name = \"nsynth-data\"\n",
        "gcs_input_path = \"nsynth-train.jsonwav.tar.gz\"\n",
        "gcs_output_path = \"nsynth-train_resampled.jsonwav.tar.gz\"\n",
        "target_sample_rate = 4000\n",
        "split = \"train\"\n",
        "\n",
        "compress_and_upload_audio(bucket_name,gcs_input_path,gcs_output_path,target_sample_rate,split)\n",
        "'''"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MEeDKla7866f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "from google.cloud import storage\n",
        "\n",
        "def download_dataset(gcs_input_path, split, bucket_name=\"nsynth-data\"):\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "\n",
        "    local_tar_path = f\"/tmp/{split}_source.tar.gz\"\n",
        "    extract_path = f\"/tmp/{split}/unzipped\"\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "    # 1. Download the tar.gz file from GCS\n",
        "    print(\"‚úÖ Starting download!\")\n",
        "    bucket.blob(gcs_input_path).download_to_filename(local_tar_path)\n",
        "\n",
        "    # 2. Extract contents to target path\n",
        "    with tarfile.open(local_tar_path, \"r:gz\") as tar:\n",
        "        tar.extractall(path=extract_path)\n",
        "\n",
        "    print(f\"‚úÖ Downloaded and extracted {gcs_input_path} to {extract_path}\")\n",
        "    return extract_path\n",
        "\n",
        "\n",
        "def json_file(json_path,split,bucket_name = \"nsynth-data\"):\n",
        "  client = storage.Client()\n",
        "  bucket = client.bucket(bucket_name)\n",
        "\n",
        "  local_json_path = f\"/tmp/{split}.json\"\n",
        "  bucket.blob(json_path).download_to_filename(local_json_path)\n",
        "  print(f\"‚úÖ Downloaded {json_path} to {local_json_path}\")\n",
        "  return local_json_path\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PYbqRYwSnlZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gcs_input_path = \"nsynth-train_resampled.jsonwav.tar.gz\"\n",
        "split = \"train\"\n",
        "bucket_name = \"nsynth-data\"\n",
        "extract_path = download_dataset(gcs_input_path, split, bucket_name)\n",
        "\n",
        "\n",
        "json_input_path =\"nsynth-train/examples.json\"\n",
        "split = \"train\"\n",
        "bucket_name = \"nsynth-data\"\n",
        "json_path = json_file(json_input_path,split,bucket_name)\n"
      ],
      "metadata": {
        "id": "xgzEkRZqpNh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Mr879v-0ZWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yld-V-Z6NSZa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torchaudio\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NSynthDataset(Dataset):\n",
        "    def __init__(self, split=\"train\", local_cache_dir=\"/tmp/\"):\n",
        "        super().__init__()\n",
        "        self.root_dir = Path(local_cache_dir) / split / \"unzipped\"\n",
        "\n",
        "        # 1. Load the JSON labels\n",
        "        label_path = Path(local_cache_dir) / f\"{split}.json\"\n",
        "        if not label_path.exists():\n",
        "            raise FileNotFoundError(f\"Label file not found: {label_path}\")\n",
        "        with open(label_path, \"r\") as f:\n",
        "            self.labels = json.load(f)\n",
        "\n",
        "        # 2. Scan all WAV files once and map stem ‚Üí full path\n",
        "        wav_paths = list(self.root_dir.rglob(\"*.wav\"))\n",
        "        if not wav_paths:\n",
        "            raise FileNotFoundError(f\"No .wav files under {self.root_dir}\")\n",
        "        self.wav_map = {p.stem: p for p in wav_paths}\n",
        "\n",
        "        # 3. Filter labels to only those with existing WAVs\n",
        "        all_keys = list(self.labels.keys())\n",
        "        self.file_names = [fn for fn in all_keys if fn in self.wav_map]\n",
        "        skipped = len(all_keys) - len(self.file_names)\n",
        "        if skipped:\n",
        "            print(f\"‚ö†Ô∏è  Skipped {skipped} label(s) without audio files\")\n",
        "\n",
        "        # 4. Velocity mapping\n",
        "        self.velocity_map = {25: 0, 50: 1, 75: 2, 100: 3, 127: 4}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fn = self.file_names[idx]\n",
        "        wav_path = self.wav_map[fn]\n",
        "\n",
        "        # Load audio\n",
        "        audio, sr = torchaudio.load(str(wav_path))\n",
        "\n",
        "        # Fetch metadata\n",
        "        m = self.labels[fn]\n",
        "        return {\n",
        "            \"pitch\":             torch.tensor(m[\"pitch\"], dtype=torch.long),\n",
        "            \"velocity\":          torch.tensor(self.velocity_map[int(m[\"velocity\"])], dtype=torch.long),\n",
        "            \"instrument_family\": torch.tensor(m[\"instrument_family\"], dtype=torch.long),\n",
        "            \"instrument_source\": torch.tensor(m[\"instrument_source\"], dtype=torch.long),\n",
        "            \"qualities\":         torch.tensor(m[\"qualities\"], dtype=torch.long),\n",
        "            \"audio\":             audio,\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXy6dmBAr8zk"
      },
      "outputs": [],
      "source": [
        "#Test Dataset and data loader\n",
        "dataset = NSynthDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True,num_workers = 2, pin_memory = True)\n",
        "batch = next(iter(dataloader))\n",
        "batch[\"pitch\"][0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWb9n9gCb4cC"
      },
      "source": [
        "##2. Create the GANsynth Model\n",
        "\n",
        "## üéõ Embedding-Based Conditioning for GAN\n",
        "\n",
        "This model uses **learned embeddings** and encoded binary vectors to represent conditioning features from the NSynth dataset. Each feature is embedded or projected into a lower-dimensional continuous space and concatenated to form a high-dimensional vector. This vector conditions both the **generator** and **discriminator** networks.\n",
        "\n",
        "### üî¢ Feature Embedding Dimensions\n",
        "\n",
        "| Feature               | Value Range                | Representation Type     | Output Dimension |\n",
        "|------------------------|-----------------------------|--------------------------|------------------|\n",
        "| **Pitch**             | 0‚Äì127 (128 classes)         | Embedding                | 18               |\n",
        "| **Velocity**          | {25, 50, 75, 100, 127} (5 classes)| Embedding                | 3                |\n",
        "| **Instrument Family** | 0‚Äì10 (11 classes)           | Embedding                | 5                |\n",
        "| **Instrument Source** | 0‚Äì2 (3 classes)             | Embedding                | 3                |\n",
        "| **Qualities**         | 10-dim binary vector        | Linear projection (`10 ‚Üí 10`) | 10              |\n",
        "\n",
        "- All categorical features use **`nn.Embedding(num_classes, embed_dim)`**.\n",
        "- The embedding dimensions are determined using the following heuristic formula:\n",
        "\n",
        "> üìê **Embedding Dimension = ‚åà1.6 √ó ‚àö(num_classes)‚åâ**\n",
        "\n",
        "- `Qualities` is a multi-label binary vector (e.g., distorted, bright, percussive).\n",
        "- It is projected to 10 dimensions using a small feedforward layer:  \n",
        "  `projected_qualities = Linear(10, 10)(qualities)`\n",
        "\n",
        "### üìê Total Conditioning Vector Size\n",
        "\n",
        "All conditioning features are concatenated into one vector:\n",
        "\n",
        "**18 (pitch) + 3 (velocity) + 5 (instrument family) + 3 (instrument source) + 10 (qualities) = 39 dimensions**\n",
        "\n",
        "This **39-dimensional conditioning vector** is concatenated with the latent noise vector `z` and passed into the generator and discriminator networks, enabling the GAN to synthesize high-quality, controllable musical notes that reflect both physical and perceptual attributes of sound.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNSVpvTA0zTO"
      },
      "source": [
        "#2.1  PixelNorm (Pixel-wise Feature Normalization)\n",
        "\n",
        "PixelNorm is a normalization layer used in GANSynth generators\n",
        "It operates **per pixel** and normalizes the **channel-wise feature vector** to have **unit average squared magnitude**.\n",
        "\n",
        "---\n",
        "\n",
        "## üìê Formula\n",
        "\n",
        "For a 4D input tensor `x` of shape `(B, C, H, W)`, the PixelNorm formula is:\n",
        "$$\n",
        "[\n",
        "\\text{PixelNorm}(x_{b,c,h,w}) = \\frac{x_{b,c,h,w}}{\\sqrt{\\frac{1}{C} \\sum_{c=1}^{C} x_{b,c,h,w}^2 + \\epsilon}}\n",
        "]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxoUrK7WRYfY"
      },
      "outputs": [],
      "source": [
        "# PixelNorm block\n",
        "# It normalizes each pixel by considering all channels at that pixel location.\n",
        "# The normalization ensures that the **average variance across channels** for each pixel is ‚âà 1.\n",
        "\n",
        "class PixelNorm(nn.Module):\n",
        "  def __init__(self,epsilon = 1e-8):\n",
        "    super().__init__()\n",
        "    self.epsilon = epsilon\n",
        "  def forward(self, x):\n",
        "    return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngwlsl9yGnIK"
      },
      "source": [
        "## üîº Upsampling √ó2 Block (GANSynth Style)\n",
        "\n",
        "This block is a core component in the generator architecture. It progressively increases the spatial resolution (height √ó width) of the feature map while refining features through convolutional layers.\n",
        "\n",
        "### üß± Block Structure\n",
        "\n",
        "The Upsampling √ó2 Block consists of the following:\n",
        "\n",
        "1. **Nearest-Neighbor Upsampling**  \n",
        "   - Doubles both the height and width of the feature map  \n",
        "   - e.g., (H, W) ‚Üí (2H, 2W)\n",
        "\n",
        "2. **Two Conv2D Layers**  \n",
        "   - Each with:\n",
        "     - `kernel_size = 3`\n",
        "     - `stride = 1`\n",
        "     - `padding = 1`\n",
        "\n",
        "\n",
        "### üîÅ Full Layer Sequence\n",
        "\n",
        "```plaintext\n",
        "Input (B, C, H, W)\n",
        "‚Üì\n",
        "Upsample (scale_factor=2) ‚Üí (B, C, 2H, 2W)\n",
        "‚Üì\n",
        "Conv2D (3√ó3, stride=1, padding=1) ‚Üí (B, C_out, 2H, 2W)\n",
        "‚Üì\n",
        "PixelNorm\n",
        "‚Üì\n",
        "LeakyReLU\n",
        "‚Üì\n",
        "Conv2D (3√ó3, stride=1, padding=1) ‚Üí (B, C_out, 2H, 2W)\n",
        "‚Üì\n",
        "PixelNorm\n",
        "‚Üì\n",
        "LeakyReLU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGPUD9oT5_Ei"
      },
      "outputs": [],
      "source": [
        "#Doubles the width and height by factor of 2\n",
        "\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.norm1 = PixelNorm()\n",
        "        self.act1 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.norm2 = PixelNorm()\n",
        "        self.act2 = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upsample(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.act1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.act2(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t3lzBHpBy0K"
      },
      "source": [
        "##2.2 Generator Architecture Implementation\n",
        "\n",
        "\n",
        "The generator progressively upsamples a low-dimensional latent vector `z`, conditioned on attributes such as pitch, instrument family, velocity, and qualities. It uses:\n",
        "\n",
        "- **Nearest-neighbor upsampling**\n",
        "- **2D convolutional blocks**\n",
        "- **PixelNorm** normalization\n",
        "- **LeakyReLU** activations\n",
        "\n",
        "Each block approximately doubles the resolution of the spectrogram representation, ending in a 2-channel output (real and imaginary components).\n",
        "\n",
        "üì∑ **The diagram of the GANSynth generator architecture will be used as a reference for implementation.**\n",
        "\n",
        "## üéõÔ∏è GANSynth Generator Architecture - Taken from original paper\n",
        "\n",
        "| Layer Description      | Output Size          | Kernel Size | Filters | Nonlinearity       |\n",
        "|------------------------|----------------------|-------------|---------|--------------------|\n",
        "| concat(Z, Pitch)       | (1, 1, 317)          | -           | -       | -                  |\n",
        "| Conv2D                 | (2, 16, 256)         | 2 √ó 16      | 256     | PN + LeakyReLU     |\n",
        "| Conv2D                 | (2, 16, 256)         | 3 √ó 3       | 256     | PN + LeakyReLU     |\n",
        "| Upsample (2√ó2)         | (4, 32, 256)         | -           | -       | -                  |\n",
        "| Conv2D                 | (4, 32, 256)         | 3 √ó 3       | 256     | PN + LeakyReLU     |\n",
        "| Conv2D                 | (4, 32, 256)         | 3 √ó 3       | 256     | PN + LeakyReLU     |\n",
        "| Upsample (2√ó2)         | (8, 64, 256)         | -           | -       | -                  |\n",
        "| Conv2D                 | (8, 64, 256)         | 3 √ó 3       | 256     | PN + LeakyReLU     |\n",
        "| Conv2D                 | (8, 64, 256)         | 3 √ó 3       | 256     | PN + LeakyReLU     |\n",
        "| Upsample (2√ó2)         | (16, 128, 256)       | -           | -       | -                  |\n",
        "| Conv2D                 | (16, 128, 256)       | 3 √ó 3       | 256     | PN + LeakyReLU     |\n",
        "| Conv2D                 | (16, 128, 256)       | 3 √ó 3       | 256     | PN + LeakyReLU     |\n",
        "| Upsample (2√ó2)         | (32, 256, 256)       | -           | -       | -                  |\n",
        "| Conv2D                 | (32, 256, 128)       | 3 √ó 3       | 128     | PN + LeakyReLU     |\n",
        "| Conv2D                 | (32, 256, 128)       | 3 √ó 3       | 128     | PN + LeakyReLU     |\n",
        "| Upsample (2√ó2)         | (64, 512, 128)       | -           | -       | -                  |\n",
        "| Conv2D                 | (64, 512, 64)        | 3 √ó 3       | 64      | PN + LeakyReLU     |\n",
        "| Conv2D                 | (64, 512, 64)        | 3 √ó 3       | 64      | PN + LeakyReLU     |\n",
        "| Upsample (2√ó2)         | (128, 1024, 64)      | -           | -       | -                  |\n",
        "| Conv2D                 | (128, 1024, 32)      | 3 √ó 3       | 32      | PN + LeakyReLU     |\n",
        "| Conv2D                 | (128, 1024, 32)      | 3 √ó 3       | 32      | PN + LeakyReLU     |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbUyK6bHjJPm"
      },
      "outputs": [],
      "source": [
        "\n",
        "class mini_Generator_v1(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding and projection layers for conditioning\n",
        "        self.pitch_embedding = nn.Embedding(128, 18)\n",
        "        self.velocity_embedding = nn.Embedding(5, 3)\n",
        "        self.instrument_family_embedding = nn.Embedding(11, 5)\n",
        "        self.instrument_source_embedding = nn.Embedding(3, 3)\n",
        "        self.quality_projection = nn.Linear(10, 10)\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.activation = nn.LeakyReLU(0.2)\n",
        "        self.pixel_norm = PixelNorm()\n",
        "\n",
        "        # Initial linear projection to shape (256, 2, 16)\n",
        "        self.initial_linear = nn.Linear(latent_dim + 39, 256 * 2 * 16)\n",
        "        self.initial_conv = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Upsampling stack (progressively increasing H, W)\n",
        "        self.upsample_layers = nn.Sequential(\n",
        "            UpsampleBlock(256, 128),\n",
        "            UpsampleBlock(128, 64),\n",
        "            UpsampleBlock(64, 256),\n",
        "            UpsampleBlock(256, 128),\n",
        "            UpsampleBlock(128, 64),\n",
        "            UpsampleBlock(64, 32)\n",
        "        )\n",
        "\n",
        "        # Final conv and output activation\n",
        "        self.last_conv = nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1)\n",
        "        self.output_activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, batch, z):\n",
        "        # Embeddings and projection\n",
        "        pitch_emb = self.pitch_embedding(batch[\"pitch\"])                     # (B, 18)\n",
        "        velocity_emb = self.velocity_embedding(batch[\"velocity\"])           # (B, 3)\n",
        "        inst_family_emb = self.instrument_family_embedding(batch[\"instrument_family\"])  # (B, 5)\n",
        "        inst_source_emb = self.instrument_source_embedding(batch[\"instrument_source\"])  # (B, 3)\n",
        "        quality_proj = self.quality_projection(batch[\"qualities\"].float())  # (B, 10)\n",
        "\n",
        "        # Combine all conditioning vectors + latent z\n",
        "        cond_vec = torch.cat([\n",
        "            pitch_emb, velocity_emb, inst_family_emb,\n",
        "            inst_source_emb, quality_proj, z\n",
        "        ], dim=1)  # (B, 167)\n",
        "\n",
        "        # Initial projection and conv\n",
        "        x = self.initial_linear(cond_vec)                   # (B, 8192)\n",
        "        x = x.view(x.size(0), 256, 2, 16)                   # (B, 256, 2, 16)\n",
        "        x = self.initial_conv(x)                            # (B, 256, 2, 16)\n",
        "        x = self.pixel_norm(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        # Upsample to final resolution\n",
        "        x = self.upsample_layers(x)                         # ‚Üí (B, 32, 128, 1024)\n",
        "\n",
        "        # Final conv + output scaling\n",
        "        x = self.last_conv(x)                               # (B, 2, 128, 1024)\n",
        "        x = self.output_activation(x)                       # [-1, 1] range\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpe68ZSDj8Tt"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "z = torch.randn(32,128).to(device)\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "generator = mini_Generator_v1().to(device)\n",
        "\n",
        "generator.eval()\n",
        "with torch.inference_mode():\n",
        "  output = generator(batch, z)         # call forward() with batch and"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkEzxBAatz3G"
      },
      "outputs": [],
      "source": [
        "output.shape # Correct shape (32,2,128,1024)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeA8eyT9vZM1"
      },
      "source": [
        "## 2.3 üéöÔ∏è Discriminator Architecture\n",
        "\n",
        "The discriminator receives the **generated (or real) spectrogram** from the generator with shape `(128, 1024, 2)`, and progressively **downsamples** it through a series of convolutional blocks. The final outputs are:\n",
        "\n",
        "- A **real/fake score**, represented as a **single scalar value** per sample indicating whether the input is real or generated.\n",
        "- A **conditioning regression head**, which predicts a **39-dimensional embedding vector** representing the original conditioning inputs (e.g., pitch, velocity, instrument family, source, and qualities).\n",
        "\n",
        "\n",
        "### üß± Block-wise Architecture\n",
        "\n",
        "| Layer Description         | Output Size         | Kernel Size | Filters | Nonlinearity       |\n",
        "|---------------------------|----------------------|-------------|---------|--------------------|\n",
        "| Input image               | (128, 1024, 2)       | -           | -       | -                  |\n",
        "| Conv2D                    | (128, 1024, 32)      | 1 √ó 1       | 32      | -                  |\n",
        "| Conv2D                    | (128, 1024, 32)      | 3 √ó 3       | 32      | LeakyReLU          |\n",
        "| Conv2D                    | (128, 1024, 32)      | 3 √ó 3       | 32      | LeakyReLU          |\n",
        "| Downsample (stride=2)     | (64, 512, 32)        | -           | -       | -                  |\n",
        "| Conv2D                    | (64, 512, 64)        | 3 √ó 3       | 64      | LeakyReLU          |\n",
        "| Conv2D                    | (64, 512, 64)        | 3 √ó 3       | 64      | LeakyReLU          |\n",
        "| Downsample (stride=2)     | (32, 256, 64)        | -           | -       | -                  |\n",
        "| Conv2D                    | (32, 256, 128)       | 3 √ó 3       | 128     | LeakyReLU          |\n",
        "| Conv2D                    | (32, 256, 128)       | 3 √ó 3       | 128     | LeakyReLU          |\n",
        "| Downsample (stride=2)     | (16, 128, 128)       | -           | -       | -                  |\n",
        "| Conv2D                    | (16, 128, 256)       | 3 √ó 3       | 256     | LeakyReLU          |\n",
        "| Conv2D                    | (16, 128, 256)       | 3 √ó 3       | 256     | LeakyReLU          |\n",
        "| Downsample (stride=2)     | (8, 64, 256)         | -           | -       | -                  |\n",
        "| Conv2D                    | (8, 64, 256)         | 3 √ó 3       | 256     | LeakyReLU          |\n",
        "| Conv2D                    | (8, 64, 256)         | 3 √ó 3       | 256     | LeakyReLU          |\n",
        "| Downsample (stride=2)     | (4, 32, 256)         | -           | -       | -                  |\n",
        "| Conv2D                    | (4, 32, 256)         | 3 √ó 3       | 256     | LeakyReLU          |\n",
        "| Conv2D                    | (4, 32, 256)         | 3 √ó 3       | 256     | LeakyReLU          |\n",
        "| Downsample (stride=2)     | (2, 16, 256)         | -           | -       | -                  |\n",
        "| **Minibatch Std. Concat** | (2, 16, 257)         | -           | -       | -                  |\n",
        "| Conv2D                    | (2, 16, 256)         | 3 √ó 3       | 256     | LeakyReLU          |\n",
        "| Conv2D                    | (2, 16, 256)         | 3 √ó 3       | 256     | LeakyReLU          |\n",
        "| **Pitch classifier**      | (1, 1, 61)           | -           | 61      | Softmax            |\n",
        "| **Discriminator output**  | (1, 1, 1)            | -           | 1       | - (linear)         |\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iile_Ly1Dp55"
      },
      "outputs": [],
      "source": [
        "class DownsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
        "        super().__init__()\n",
        "        self.downsample = nn.Sequential(\n",
        "            # Downsampling: keep channels, reduce spatial size\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding),  # (B, C, H, W) -> (B, C, H/2, W/2)\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            # Refinement: increase channels, preserve spatial size\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),  # (B, C, H/2, W/2) -> (B, C_out, H/2, W/2)\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.downsample(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HU2LJMhMBwlM"
      },
      "outputs": [],
      "source": [
        "class mini_Discriminator_v1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # Initial convolutional block\n",
        "    self.initial_block = nn.Sequential(\n",
        "        nn.Conv2d(2, 32, kernel_size=1),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "\n",
        "    # Downsampling stack\n",
        "    self.downsample_block = nn.Sequential(\n",
        "        DownsampleBlock(32, 64),     # 128√ó1024 ‚Üí 64√ó512\n",
        "        DownsampleBlock(64, 128),    # 64√ó512 ‚Üí 32√ó256\n",
        "        DownsampleBlock(128, 256),   # 32√ó256 ‚Üí 16√ó128\n",
        "        DownsampleBlock(256, 256),   # 16√ó128 ‚Üí 8√ó64\n",
        "        DownsampleBlock(256, 256)    # 8√ó64 ‚Üí 4√ó32\n",
        "    )\n",
        "\n",
        "    # Final spatial downsampling\n",
        "    self.final_downsample = nn.Sequential(\n",
        "        nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),  # ‚Üí 2√ó16\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "\n",
        "    # Convolution after concatenating std channel\n",
        "    self.final_conv = nn.Sequential(\n",
        "        nn.Conv2d(257, 256, kernel_size=3, padding=1),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "\n",
        "    # Conditioning regressor (outputs 39D vector)\n",
        "    self.cond_regressor = nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d((1, 1)),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(256, 39)\n",
        "    )\n",
        "\n",
        "    # Real/fake score classifier\n",
        "    self.real_classifier = nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d((1, 1)),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(256, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.initial_block(x)\n",
        "    x = self.downsample_block(x)\n",
        "    x = self.final_downsample(x)\n",
        "\n",
        "    # Append mean of std channel across batch\n",
        "    x_std = torch.std(x, dim=0, keepdim=True).mean()\n",
        "    x_std = x_std.expand(x.shape[0], 1, x.shape[2], x.shape[3])\n",
        "    x = torch.cat([x, x_std], dim=1)\n",
        "\n",
        "    x = self.final_conv(x)\n",
        "\n",
        "    x_cond = self.cond_regressor(x)       # ‚Üí (B, 39)\n",
        "    x_classifier = self.real_classifier(x)  # ‚Üí (B, 1)\n",
        "\n",
        "    return {\n",
        "    \"condition\": x_cond,       # 39-dimensional conditioning vector\n",
        "    \"real_fake\": x_classifier  # scalar real/fake score\n",
        "    }\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UQZM3liCpa4"
      },
      "outputs": [],
      "source": [
        "discriminator = mini_Discriminator_v1().to(device)\n",
        "discriminator.eval()\n",
        "with torch.inference_mode():\n",
        "  final_output = discriminator(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWe68gXwQnVY"
      },
      "outputs": [],
      "source": [
        "print(f\"The conditioning tensor has shape {final_output['condition'][0].shape}\")\n",
        "print(f\"The classifier tensor has shape {final_output['real_fake'][0].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OUxOiliW7mm"
      },
      "outputs": [],
      "source": [
        "#Make spectograms\n",
        "\n",
        "#Takes waveform: (B,1,64000) -> ((B,2,128, 1024))\n",
        "def make_spectrogram(waveform, sample_rate=16000):\n",
        "\n",
        "  waveform = waveform.squeeze(1)\n",
        "\n",
        "  # As used in GanSynth\n",
        "  n_fft = 1024\n",
        "  hop_length = 256\n",
        "  n_mels = 128\n",
        "\n",
        "  #Hann window to make audio windows smoother\n",
        "  window = torch.hann_window(n_fft).to(device)\n",
        "\n",
        "  # 1. Apply STFT on waveform`\n",
        "  stft = torch.stft(\n",
        "        input=waveform,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        return_complex=True,\n",
        "        window = window,\n",
        "    )  # shape: (B, F, T)\n",
        "\n",
        "  #2 . Log-Mel Spectogram\n",
        "  mel_filter = torchaudio.transforms.MelScale(\n",
        "    n_mels=n_mels,\n",
        "    sample_rate=sample_rate,\n",
        "    n_stft=stft.shape[1]  # usually 513 if n_fft=1024\n",
        "    ).to(device)\n",
        "\n",
        "  magnitude = abs(stft)\n",
        "  log_mel_spec = torch.log(1e-8 + mel_filter(magnitude))\n",
        "\n",
        "  #3. Phase spectogram\n",
        "  phase = torch.angle(stft)\n",
        "\n",
        "  #4. Unwrapped Phase\n",
        "  diff = torch.diff(phase, dim=-1)\n",
        "  diff_wrapped = (diff + torch.pi) % (2 * torch.pi) - torch.pi\n",
        "  correction = diff_wrapped - diff\n",
        "  correction[(diff_wrapped == -torch.pi) & (diff > 0)] = torch.pi\n",
        "  unwrapped = torch.cumsum(torch.cat([phase[..., :1], diff + correction], dim=-1), dim=-1)\n",
        "\n",
        "  #5. Instaneous phase\n",
        "  dphase = torch.diff(unwrapped, dim=-1)\n",
        "  inst_freq = dphase * sample_rate / (2 * torch.pi * hop_length)\n",
        "  inst_freq = torch.nn.functional.pad(inst_freq, (1, 0))\n",
        "\n",
        "\n",
        "  #6. Resize\n",
        "  log_mel_spec = log_mel_spec.unsqueeze(1)\n",
        "  inst_freq = inst_freq.unsqueeze(1)\n",
        "\n",
        "  log_mel_spec = torch.nn.functional.interpolate(log_mel_spec, size=(128, 1024), mode='bilinear', align_corners=False)\n",
        "  inst_freq = torch.nn.functional.interpolate(inst_freq, size=(128, 1024), mode='bilinear', align_corners=False)\n",
        "\n",
        "  #7. Concat\n",
        "  spectograms = torch.concat((log_mel_spec, inst_freq), dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return spectograms\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udAB8ATlexNY"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "audio_test = batch['audio'].to(device)\n",
        "make_spectrogram(audio_test).shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW3VcldgQpn0"
      },
      "source": [
        "## Section 3: Training Loop\n",
        "\n",
        "This section describes the training process for a GAN-based audio synthesis model. The generator is conditioned on musical attributes (pitch, velocity, etc.), and the discriminator is trained to both classify real vs. fake data and predict a semantic embedding vector corresponding to the input pitch. This embedding-based auxiliary supervision helps structure the discriminator‚Äôs internal representation space.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Training Strategy Overview\n",
        "\n",
        "Each training step involves:\n",
        "1. The **discriminator** is trained to:\n",
        "   - Distinguish real from fake data using adversarial loss,\n",
        "   - Predict a 39-dimensional embedding vector corresponding to the input pitch (using MSE loss).\n",
        "2. The **generator** is trained to:\n",
        "   - Fool the discriminator (adversarial loss),\n",
        "   -  Produce outputs that correct pitch embeddings when passed to the discriminator (MSE loss).\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Loss Functions\n",
        "\n",
        "#### 1. **Discriminator Loss**\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_D = \\mathcal{L}_{\\text{adv}}^{\\text{real}} + \\mathcal{L}_{\\text{adv}}^{\\text{fake}} + \\lambda_{\\text{aux}} \\cdot \\mathcal{L}_{\\text{embed}}^{\\text{real}}\n",
        "$$\n",
        "\n",
        "- **Adversarial loss** (binary cross-entropy):\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{adv}} = -[y \\cdot \\log(x) + (1 - y) \\cdot \\log(1 - x)]\n",
        "$$\n",
        "\n",
        "- **Auxiliary embedding loss** (mean squared error):\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{embed}}^{\\text{real}} = \\text{MSE}(D_{\\text{embed}}(x_{\\text{real}}), \\text{PitchEmbed}(pitch_{\\text{true}}))\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Generator Loss**\n",
        "\n",
        "Two versions:\n",
        "\n",
        "- **Basic Generator Loss (default):**\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_G = \\mathcal{L}_{\\text{adv}}^{\\text{fake}}\n",
        "$$\n",
        "\n",
        "- **Extended Generator Loss (with embedding supervision):**\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_G = \\mathcal{L}_{\\text{adv}}^{\\text{fake}} + \\lambda_{\\text{aux}} \\cdot \\mathcal{L}_{\\text{embed}}^{\\text{fake}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{embed}}^{\\text{fake}} = \\text{MSE}(D_{\\text{embed}}(G(z)), \\text{PitchEmbed}(pitch_{\\text{true}}))\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bwzt_OJ7DnT8"
      },
      "outputs": [],
      "source": [
        "#Define training loop\n",
        "def train_loop(generator,discriminator, train_loader, criterion_adv,criterion_embed, optimizer_G,optimizer_D, device, lambda_embed = 1, epochs=200):\n",
        "  generator.train()\n",
        "  discriminator.train()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    generator_loss = 0\n",
        "    discriminator_loss = 0\n",
        "    for batch_idx, cond in enumerate(train_loader):\n",
        "      z = torch.randn(32,128).to(device) #Random noise\n",
        "      cond = {k: v.to(device) for k, v in cond.items()}\n",
        "      waveform_real = cond['audio'].to(device)\n",
        "\n",
        "      #Get embedded_vector\n",
        "      embedded_vector = torch.cat([  # üîß changed torch.concat ‚Üí torch.cat\n",
        "          generator.pitch_embedding(cond['pitch']),\n",
        "          generator.velocity_embedding(cond['velocity']),\n",
        "          generator.instrument_family_embedding(cond['instrument_family']),\n",
        "          generator.instrument_source_embedding(cond['instrument_source']),\n",
        "          generator.quality_projection(cond['qualities'].float()),\n",
        "      ],dim = 1)\n",
        "\n",
        "      embedded_vector_D = embedded_vector.detach()\n",
        "\n",
        "      #Create spectogram\n",
        "      spectogram_real = make_spectrogram(waveform_real)\n",
        "      spectogram_generated = generator(cond, z)\n",
        "\n",
        "      discriminator_real = discriminator(spectogram_real)\n",
        "      discriminator_generated = discriminator(spectogram_generated.detach())\n",
        "      #Train discriminator\n",
        "\n",
        "      discriminator_real_classifier = discriminator_real['real_fake']\n",
        "      discriminator_real_embed = discriminator_real['condition']\n",
        "      discriminator_generated_classifier = discriminator_generated['real_fake']\n",
        "      discriminator_generated_embed = discriminator_generated['condition']\n",
        "      #Generate labels\n",
        "      real_labels = torch.ones_like(discriminator_real_classifier).detach().to(device)\n",
        "      generated_labels = torch.zeros_like(discriminator_generated_classifier).detach().to(device)\n",
        "\n",
        "      #Adversarial loss\n",
        "      loss_adversarial_discriminator = criterion_adv(discriminator_real_classifier,real_labels) + criterion_adv(discriminator_generated_classifier,generated_labels)\n",
        "      #Auxiliary embedding loss\n",
        "      loss_embed_discriminator = criterion_embed(discriminator_real_embed, embedded_vector_D)\n",
        "      total_discriminator_loss = loss_adversarial_discriminator + lambda_embed * loss_embed_discriminator #Total discriminator loss\n",
        "\n",
        "      optimizer_D.zero_grad()\n",
        "      total_discriminator_loss.backward()\n",
        "      optimizer_D.step()\n",
        "      del spectogram_real, discriminator_real, discriminator_generated\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      print(\"Discriminator trained!\")\n",
        "\n",
        "      #Train Generator\n",
        "      discriminator_generated = discriminator(spectogram_generated)  # re-run for G\n",
        "      discriminator_generated_classifier = discriminator_generated['real_fake']  # üîß added: re-extract outputs for generator step\n",
        "      discriminator_generated_embed = discriminator_generated['condition']\n",
        "      loss_adversarial_generator = criterion_adv(discriminator_generated_classifier, real_labels)\n",
        "      loss_embed_generator = criterion_embed(discriminator_generated_embed, embedded_vector)\n",
        "      total_generator_loss = loss_adversarial_generator + lambda_embed * loss_embed_generator #Total generator loss\n",
        "\n",
        "      optimizer_G.zero_grad()\n",
        "      total_generator_loss.backward()\n",
        "      optimizer_G.step()\n",
        "      del spectogram_generated, discriminator_generated_embed, discriminator_generated_classifier\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      print(\"Generator trained!\")\n",
        "      #Log generator and discriminator loss\n",
        "      generator_loss += total_generator_loss.item()\n",
        "      discriminator_loss += total_discriminator_loss.item()\n",
        "\n",
        "      print(f\"[Epoch {epoch + 1}/{epochs}] \"\n",
        "              f\"Generator Loss: {generator_loss:.4f} | \"\n",
        "              f\"Discriminator Loss: {discriminator_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2jRpBw821dI"
      },
      "outputs": [],
      "source": [
        "\n",
        "#device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Define models\n",
        "generator = mini_Generator_v1().to(device)\n",
        "discriminator = mini_Discriminator_v1().to(device)\n",
        "\n",
        "#Define loss functions\n",
        "criterion_adv = nn.BCEWithLogitsLoss()\n",
        "criterion_embed = nn.MSELoss()\n",
        "\n",
        "# Define optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
        "\n",
        "#Dataset and Dataloader\n",
        "dataset = NSynthDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True,num_workers = 2, pin_memory = True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ut-Ruaq220J",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "#Training loop\n",
        "train_loop(generator, discriminator, dataloader, criterion_adv, criterion_embed,\n",
        "           optimizer_G, optimizer_D, device, lambda_embed=1.0, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNswbDJ7VbDl"
      },
      "outputs": [],
      "source": [
        "#This is too heavy to run for me. Refer to GanSynth_mini"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qUZ0A4923SL5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMnRqq8u9ftL2zoHjxRBEhi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}